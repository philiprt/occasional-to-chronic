{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac0ffb2-f462-443a-b683-20cabf8363ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib widget\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=RuntimeWarning)\n",
    "\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from station_analysis import station_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108a9569",
   "metadata": {},
   "source": [
    "# Load locations and meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f710bca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"./data/tide_gauge_locations.csv\"\n",
    "\n",
    "# load the metadata file if it already exists\n",
    "if os.path.exists(fn):\n",
    "    locations = pd.read_csv(\n",
    "        fn,\n",
    "        dtype=dict(\n",
    "            station_country_code=int,\n",
    "            uhslc_id=int,\n",
    "        ),\n",
    "        index_col=\"uhslc_id\",\n",
    "    )\n",
    "\n",
    "# otherewise create it\n",
    "else:\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # get basic metadata from UHSLC server\n",
    "    locations = pd.read_csv(\n",
    "        \"https://uhslc.soest.hawaii.edu/erddap/tabledap/global_hourly_fast.csv?latitude%2Clongitude%2Cstation_name%2Cstation_country%2Cstation_country_code%2Cuhslc_id&time%3E=2023-12-24T00%3A00%3A00Z&time%3C=2023-12-31T22%3A59%3A59Z&distinct()\",\n",
    "        dtype=dict(station_country_code=\"Int64\", uhslc_id=\"Int64\"),\n",
    "        index_col=\"uhslc_id\",\n",
    "    )\n",
    "    locations = locations.drop(locations.index == pd.NA)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # add alpha-3 country codes\n",
    "    country_codes = pd.read_csv(\n",
    "        \"./data/iso_3166_country_codes.csv\", dtype=dict(numeric=int), index_col=5\n",
    "    )\n",
    "    locations[\"station_country_alpha3\"] = None\n",
    "    for uhid in locations.index:\n",
    "        num_cc = locations.loc[uhid, \"station_country_code\"]\n",
    "        if num_cc in country_codes.index:\n",
    "            locations.loc[uhid, \"station_country_alpha3\"] = country_codes.loc[\n",
    "                num_cc, \"alpha_3\"\n",
    "            ]\n",
    "        else:\n",
    "            locations.loc[uhid, \"station_country_alpha3\"] = None\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # get hdi values\n",
    "    def get_hdi_from_api(a3):\n",
    "        response = requests.get(\n",
    "            f\"https://api.hdrdata.org/CountryIndicators/filter?country={a3}&year=2021&indicator=hdi\"\n",
    "        ).json()\n",
    "        return response[0][\"value\"] if len(response) > 0 else None\n",
    "\n",
    "    locations[\"hdi\"] = None\n",
    "    for uhid in locations.index:\n",
    "        cca3 = locations.loc[uhid, \"station_country_alpha3\"]\n",
    "        locations.loc[uhid, \"hdi\"] = get_hdi_from_api(cca3)\n",
    "        # if the api returns no hdi value, try getting the hdi of the sovereign\n",
    "        if locations.loc[uhid, \"hdi\"] is None:\n",
    "            sov = country_codes.loc[\n",
    "                locations.loc[uhid, \"station_country_code\"], \"sovereignty\"\n",
    "            ]\n",
    "            sov_cca3 = [\n",
    "                c.alpha_3\n",
    "                for _, c in country_codes.iterrows()\n",
    "                if c[\"name\"][: len(sov)] == sov\n",
    "            ]\n",
    "            if len(sov_cca3) > 0:\n",
    "                sov_cca3 = sov_cca3[0] if \"USA\" not in sov_cca3 else \"USA\"\n",
    "                locations.loc[uhid, \"hdi\"] = get_hdi_from_api(sov_cca3)\n",
    "\n",
    "    # --------------------------------------------------------------------\n",
    "    # save\n",
    "    locations.to_csv(fn, index=True)\n",
    "\n",
    "locations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8906ddcb-0935-4815-a54e-570b5f7ff432",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Perform station analysis \n",
    "Analyze the global set of stations. This step includes calcuation of $\\Delta h$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a4b240",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_hours_per_day = 20\n",
    "min_days_per_year = 320\n",
    "min_years_for_inclusion = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f00a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "qc_fig_dir = \"./figures/quality_control/\"\n",
    "os.makedirs(qc_fig_dir, exist_ok=True)\n",
    "\n",
    "tide_prd_dir = \"./data/tide_predictions/\"\n",
    "os.makedirs(tide_prd_dir, exist_ok=True)\n",
    "\n",
    "ga_file = \"./output/global_analysis.csv\"\n",
    "os.makedirs(\"./output\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c03624",
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.exists(ga_file):\n",
    "    global_analysis = pd.read_csv(ga_file, index_col=0)\n",
    "    global_analysis.index.name = \"uhid\"\n",
    "else:\n",
    "    global_analysis = None\n",
    "\n",
    "for n, (uhid, tg) in enumerate(locations.iterrows()):\n",
    "    if global_analysis is not None and uhid in global_analysis.index:\n",
    "        continue\n",
    "    print(\n",
    "        f\"Location {n+1} of {locations.index.size}\",\n",
    "        end=\"\\r\" if n + 1 < locations.index.size else \"\\n\\n\",\n",
    "    )\n",
    "    analysis = station_analysis(\n",
    "        tg,\n",
    "        min_hours_per_day,\n",
    "        min_days_per_year,\n",
    "        min_years_for_inclusion,\n",
    "        tide_prd_dir,\n",
    "        qc_fig_dir,\n",
    "    )\n",
    "    if analysis is not None:\n",
    "        if global_analysis is None:\n",
    "            dtypes = {\n",
    "                c: (\n",
    "                    int\n",
    "                    if c in [\"hdi\", \"n_good_years\"]\n",
    "                    else (object if c == \"name\" else float)\n",
    "                )\n",
    "                for c in analysis.index\n",
    "            }\n",
    "            global_analysis = pd.DataFrame(columns=analysis.index)\n",
    "            global_analysis = global_analysis.astype(dtypes)\n",
    "            global_analysis.index.name = \"uhid\"\n",
    "        global_analysis.loc[uhid, :] = analysis\n",
    "\n",
    "    global_analysis.to_csv(ga_file, index=True)\n",
    "\n",
    "global_analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42be621c",
   "metadata": {},
   "source": [
    "# Quick comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62dd99d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \"dh_median_26_days\"\n",
    "c2 = [\n",
    "    \"tide_dymx_std\",\n",
    "    \"res_momn_std\",\n",
    "    \"res_hf_dymx_std\",\n",
    "]\n",
    "\n",
    "print(f\"Correlation between {c1} and ...\")\n",
    "for c2i in c2:\n",
    "    r = np.corrcoef([global_analysis[c1], global_analysis[c2i]])[0, 1]\n",
    "    print(f\"{c2i}: {r}\")\n",
    "\n",
    "plt.figure()\n",
    "for c2i in c2:\n",
    "    plt.plot(global_analysis[c1], global_analysis[c2i], \"o\", label=c2i)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f983922",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1 = \"dh_median_26_days\"\n",
    "c2 = [\n",
    "    \"res_momn_std\",\n",
    "    \"res_momn_q75_std\",\n",
    "    \"res_momn_amx_std\",\n",
    "]\n",
    "\n",
    "print(f\"Correlation between {c1} and ...\")\n",
    "for c2i in c2:\n",
    "    r = np.corrcoef([global_analysis[c1], global_analysis[c2i]])[0, 1]\n",
    "    print(f\"{c2i}: {r}\")\n",
    "\n",
    "plt.figure()\n",
    "for c2i in c2:\n",
    "    plt.plot(global_analysis[c1], global_analysis[c2i], \"o\", label=c2i)\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cc9cee",
   "metadata": {},
   "source": [
    "# Temporal analysis\n",
    "This step includes calculation of $\\Delta t$ and SLR contributions.\n",
    "\n",
    "First load the gridded Interagency SLR projections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d322841",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr = xr.open_dataset(\"./data/slr_scenarios/TR_gridded_projections.nc\")\n",
    "massdef_components = [\"AIS\", \"GIS\", \"glaciers\", \"landwaterstorage\"]\n",
    "for scn in [\"Low\", \"IntLow\", \"Int\", \"IntHigh\", \"High\"]:\n",
    "    slr = xr.merge(\n",
    "        [\n",
    "            slr,\n",
    "            xr.concat(\n",
    "                [slr[f\"rsl_{c}_{scn}\"] for c in massdef_components], dim=\"component\"\n",
    "            )\n",
    "            .rename(f\"rsl_massanddeformation_{scn}\")\n",
    "            .sum(dim=\"component\"),\n",
    "        ]\n",
    "    )\n",
    "slr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b77f8c",
   "metadata": {},
   "source": [
    "Then stack the lon/lat coordinates and isolate ocean points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb716e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "slr_stack = slr.stack(dict(location=(\"lon\", \"lat\")))\n",
    "ocean = slr_stack.rsl_total_Int.isel(years=0, percentiles=1).values > -3e4\n",
    "slr_stack = slr_stack.sel(location=ocean)\n",
    "slr_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9655502",
   "metadata": {},
   "source": [
    "Loop over each station and perform the temporal calculations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb65b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_names = [\"Low\", \"IntLow\", \"Int\", \"IntHigh\", \"High\"]\n",
    "dh_days = [c for c in global_analysis.columns if c[:2] == \"dh\"]\n",
    "dt_start_years = [2020, 2030, 2040, 2050, 2060]\n",
    "slr_duration = 30  # years\n",
    "\n",
    "t_analysis = dict()\n",
    "for tgi, tg in global_analysis.iterrows():\n",
    "\n",
    "    t_analysis[tgi] = pd.Series()\n",
    "\n",
    "    # isolate the the closest location in the SLR grid\n",
    "    closest_ocean = np.sqrt(\n",
    "        (slr_stack.lon - tg.lon) ** 2 + (slr_stack.lat - tg.lat) ** 2\n",
    "    ).idxmin()\n",
    "\n",
    "    # get the time series of total SLR for the closest grid cell\n",
    "    var_names = [f\"rsl_total_{s}\" for s in scn_names]\n",
    "    tg_slr = (\n",
    "        slr_stack.sel(\n",
    "            lon=closest_ocean.lon, lat=closest_ocean.lat, percentiles=50\n",
    "        ).to_pandas()[var_names]\n",
    "        / 10  # cm\n",
    "    )\n",
    "    tg_slr.columns = scn_names\n",
    "\n",
    "    # interpolate to annual resolution\n",
    "    annual_index = range(int(slr_stack.years[0]), int(slr_stack.years[-1]) + 1)\n",
    "    tg_slr = tg_slr.reindex(annual_index)\n",
    "    tg_slr = tg_slr.interpolate(method=\"cubicspline\")\n",
    "\n",
    "    # loop over the dh values, start years, and scenarios; calculate and tabulate dt\n",
    "    for dh in dh_days:\n",
    "        dt_str_base = f\"dt{dh[2:]}\"\n",
    "        for sy in dt_start_years:\n",
    "            dt = (tg_slr - tg_slr.loc[sy] >= tg[dh]).idxmax(axis=0) - sy\n",
    "            # dt values <= zero mean the transition never occurs due to, for\n",
    "            # example, uplift; assign such transtions to have infinite duration\n",
    "            dt.loc[dt <= 0] = np.inf\n",
    "            for scn in scn_names:\n",
    "                dt_str = f\"{dt_str_base}_{scn}_{sy}\"\n",
    "                t_analysis[tgi].loc[dt_str] = dt.loc[scn]\n",
    "\n",
    "    # get change in each contribution to SLR during intervals following the dt start yrs\n",
    "    tg_slr = slr_stack.sel(\n",
    "        lon=closest_ocean.lon, lat=closest_ocean.lat, percentiles=50\n",
    "    ).to_pandas()\n",
    "    for y0 in dt_start_years:\n",
    "        for scn in scn_names:\n",
    "            components = dict(\n",
    "                total=f\"rsl_total_{scn}\",\n",
    "                ocean_dyn=f\"rsl_oceandynamics_{scn}\",\n",
    "                vlm=f\"rsl_verticallandmotion_{scn}\",\n",
    "                ais=f\"rsl_AIS_{scn}\",\n",
    "                gis=f\"rsl_GIS_{scn}\",\n",
    "                glaciers=f\"rsl_glaciers_{scn}\",\n",
    "                landwater=f\"rsl_landwaterstorage_{scn}\",\n",
    "                massdef=f\"rsl_massanddeformation_{scn}\",\n",
    "            )\n",
    "            for c in components:\n",
    "                quantity = f\"slr_{c}_{scn}_{y0}_{y0 + slr_duration}\"\n",
    "                if quantity not in global_analysis.columns:\n",
    "                    global_analysis[quantity] = None\n",
    "                t_analysis[tgi].loc[quantity] = (\n",
    "                    tg_slr[components[c]].loc[[y0, y0 + slr_duration]].diff().values[1]\n",
    "                    / 10  # cm\n",
    "                )\n",
    "\n",
    "# make a dataframe from the temporal analysis\n",
    "t_analysis_df = pd.DataFrame(t_analysis).T\n",
    "\n",
    "# replace existing t_analysis columns in global_analysis; append new ones\n",
    "new_ga_columns = []\n",
    "for tc in t_analysis_df.columns:\n",
    "    if tc in global_analysis.columns:\n",
    "        global_analysis[tc] = t_analysis_df[tc]\n",
    "    else:\n",
    "        new_ga_columns.append(tc)\n",
    "global_analysis = pd.concat([global_analysis, t_analysis_df[new_ga_columns]], axis=1)\n",
    "\n",
    "# save and show\n",
    "global_analysis.to_csv(ga_file, index=True)\n",
    "global_analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b76843f6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
